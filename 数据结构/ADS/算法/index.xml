<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/NP%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/NP%E9%97%AE%E9%A2%98/</guid>
      <description>NP问题#回顾#FDS中提到的问题：
欧拉回路：不重复地遍历所有边。使用DFS算法，简单。 哈密顿回路：不重复地遍历所有点。困难。 单源无权重最短路径问题：使用BFS算法，简单。 单源无权重最长路径问题：困难。 “困难”表示尚未得知能保证在多项式时间内解决的算法。
问题的难易程度#最简单：$O(N)$线性时间
最困难：不能解决问题。（undecidable problems）
Decidability：可判定性。
Kurt Godel证明当前的系统不完备，即存在无法用现有定理解释的问题。一种可选方法是每遇到一个此类问题，都将其定义为true或false，但同样可以证明这种操作是无限的。
举例：
图灵停机问题（halting problem）：是否能使C编译器检查出所有无限循环？
答案：不能。
注意：停机问题是不可解问题，不是NPC或NP问题。
证明：实质上是无法证明Loop(Loop)的结果是循环还是不循环。
时间复杂度#时间复杂度分为两种级别，其中后者的复杂度无论如何都远远大于前者：
一种是$O(n)$，$O(logn)$，$O(n^a)$等，称为多项式级的复杂度，因为它的规模n出现在底数的位置；
另一种是$O(a^n)$和$O(n!)$型复杂度，属于非多项式级，其复杂度计算机往往不能承受。
概念引入#确定性图灵机（Deterministic Turing Machine）：根据当前的指令和状态选择唯一的下一条要执行的指令。 非确定性图灵机（Nondeterministic Turing Machine）：可以在一定集合中任意选择下一步，如果存在一种选择能最终导向结果，图灵机会做出正确的选择。（即非常聪明的图灵机） 定义#P问题是可以通过确定性图灵机，在多项式时间内得出结果的问题。
NP问题是可以通过非确定性图灵机，在多项式时间内得出结果的问题。
即如果能够在多项式时间内证明一个解是正确的，那么非确定性图灵机一定能选择出正确结果。（easy to check）
举例#哈密顿通路问题，可以在$O(N)$时间内验证一个回路是否不重复地遍历了所有结点，因此是一个NP问题。 但不是所有可选择的问题（decidable problems）都是NP问题。例如，判断一个图是否存在哈密顿通路不属于NP问题。
分类#P类问题：存在多项式时间算法的问题。 (Polynominal，多项式)
NP问题：能在多项式时间内验证得出一个正确解的问题。 (Nondeterministic Polynominal，非确定性多项式)
==P类问题是NP问题的子集，因为存在多项式时间解法的问题，总能在多项式时间内验证他。==
通俗来讲，不知道这个问题是不是存在多项式时间内的算法，所以叫non-deterministic非确定性，但是我们可以在多项式时间内验证并得出这个问题的一个正确解。（注意是不知道而不是不存在）
约化（Reducibility）：具有传递性，如A约化到B，B约化到C，A就可以约化到C，同时不断约化下去，可知一定会存在一个最大的问题，只需要解决了这个问题，那其下的所有问题也就解决啦。如一元一次方程可以约化成一元二次方程。
引到NP问题里就是，对于同一类的所有的NP类问题，若他们都可以在多项式时间内约化成最难的一个NP类问题，（我们直观的认为，被约化成的问题应具有比前一个问题更复杂的时间复杂度）当我们针对这个时间复杂度最高的超级NP问题要是能找到他的多项式时间算法的话，那就等于变向的证明了其下的所有问题都是存在多项式算法的，即NP=P。
NPC问题：存在这样一个NP问题，所有的NP问题都可以约化成它。 （Nondeterminism Polynomial Complete，完全NP问题）
格式化定义：
对于一个最优问题，存在两种模式，normal version需要回答出最优解，而decision version仅仅判断是或否。
==需要满足两个条件：①是一个NP问题 ②所有NP问题可以在多项式时间内约化成它。==</description>
    </item>
    
    <item>
      <title></title>
      <link>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/</guid>
      <description>分治算法#</description>
    </item>
    
    <item>
      <title></title>
      <link>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</guid>
      <description>动态规划（DP）#动态规划算法与分治法类似，其基本思想也是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。若用分治法来解这类问题，则分解得到的子问题数目太多，有些子问题被重复计算了很多次。如果我们能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。我们可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其结果填入表中。这就是动态规划法的基本思路。
步骤：
找到最优子结构 递归地定义状态转移方程（最终结果与子结构结果的关系） 找到子问题的最优解 核心：使用表代替递归。
time memory tradeoff：
例一：斐波那契数列#$$ F(N)=F(N-1)+F(N-2) $$
递归存在的问题：F0、F1、F2……重复计算多次，导致时间复杂度很高。
解决方式：保存前两步的斐波那契数，每步依次更新。
例二：矩阵相乘优先排序#不同顺序计算所需的时间不同。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2/</guid>
      <description>局部搜索（Local Search）#同样是使用近似方法解决问题。特点是关注局部最优解。
初始猜测 → 搜索局部最优解 → 用局部最优解替换上一步猜测 → 循环。
需要考虑的要素：①初始点的选择 ②步长的选择
是否能在有限步数内终止程序？
概念#Local#定义neighborhoods为当前点周围一个可以达到的集合。 局部最优解为neighborhoods集合中的最优选择。 Search#从一个可行解开始，找到其neighborhoods中的最优解。 若没有更优则停止程序。 邻域关系#$S\sim S&amp;rsquo;$：$S&amp;rsquo;$是$S$的邻域内的解（neighboring solution）。即$S$可以通过小范围修改得到$S&amp;rsquo;$。
$N(S)$表示$S$的邻域，即集合${S&amp;rsquo;:S\sim S&amp;rsquo;}$。
Gradient descent：梯度下降法。
实例1：Vertex Cover问题#Decision Version：
给出无向图$G=(V,E)$和整数$K$，是否存在节点集合$V&amp;rsquo;\subseteq V$使得$|V&amp;rsquo;|$最大为$K$，且$G$中的每条边至少有至少有一个节点属于$V&amp;rsquo;$？
Optimazition Version：
给出无向图$G=(V,E)$，找到一个最小集合$S\subseteq V$使得$G$中的每条边至少有至少有一个节点属于$S$？
要素：
可行解集合$FS$：所有满足vertex covers的解集合。显然$V\in FS$。
开销：$cost(S)=|S|$。
局部：$S&amp;rsquo;$可以由$S$加或减一个节点得到。由于要使开销最小，因此选择“减”一个节点。
每个$S$最多有$|V|$个邻域（删除每个节点）。
搜索：从$S=V$开始，删除一个节点，并判断$S&amp;rsquo;$是否是开销更小的vertex cover。
Case 0：只有节点没有边#只有一个全局最优解。此时S为空集。
Case 1：中心节点cover所有边#显然全局最优解为只有中心节点的集合。但若从所有节点开始，第一次就删除了中心节点，则无法删除剩余任何一个节点。
Case 2：节点直线排列没有支路#显然中间三个节点为开销最少的vertex cover。但若第一步删除了这三个节点之一，无法删除其他两个节点。
**存在的问题：**无法撤回上一步操作，很容易陷入局部最优后终止程序。
改进：Metropolis算法。
Metropolis算法#将“删除一个节点”修改为随机选择邻域内所有解，即可以增加节点。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E5%B9%B6%E8%A1%8C%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E5%B9%B6%E8%A1%8C%E7%AE%97%E6%B3%95/</guid>
      <description>并行算法（Parallel Algorithms）#概述#并行分类#机器级并行（硬件） 处理器并行 流水线 超长指令字（VLIW） 算法级并行 描述并行算法的两个模型#Parallel Random Access Machine（PRAM） Work-Depth（WD) PRAM模型#$n$个进程，共享内存。一个箭头表示一个单位时间访问（读/写/计算）。
定义processor i为$c=a+b$。如下代码表示并行计算$A(i)=B(i)$。其中==pardo==为并行运算的关键字。
for Pi, 1 &amp;lt;= i &amp;lt;= n pardo A(i) := B(i) 存在问题：多个进程同时写同一个内存块。
解决访问冲突
Exclusive-Read Exclusive-Write（EREW）：同时只允许一个进程读或写。 Concurrently-Read Exclusive-Write（CREW）：可以同时读，禁止同时写。 Concurrently-Read Concurrently-Write（CRCW）：可以同时读或写。 Arbitrary rule：随机选择一个进程执行。 Priority rule（P with the smallest number）： 按优先级选择进程执行。 Common rule：当所有进程写的是同一个值的时候允许。 实例1：加和计算
输入$A(1),A(2),\ldots,A(n)$，输出$A(1)+A(2)+\ldots+A(n)$。
从底层到顶层计算，形成完全二叉树，规律如下：
代码表示：共执行了$log n+2$次操作，每次操作为常数时间。
时间表示：
==标准的PRAM：==每个处理器都看作执行一条指令，即使是idle状态。
==实际上的：==只有部分处理器在工作。
因此标准PRAM存在一些弱点：
没有揭示算法将如何运行在不同数量的处理器上。即加上或减去一些处理器会对算法有怎样的影响。 完全指定处理器的指令分配需要一定程度的细节。 WD模型#每个处理器执行不同条数的指令。
衡量并行算法的性能#work load： 总操作数$W(n)$。 最坏情况运行时间：$T(n)$。 实例1：Prefix-Sums#输入$A(0),A(1),\ldots,A(n)$，输出$\Sigma_{i=1}^{1}A(i),\Sigma_{i=1}^{2}A(i),\ldots,\Sigma_{i=1}^{n}A(i)$。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95/</guid>
      <description>贪婪算法#概述#在Dijkstra算法求解最短路径、Kruskal算法求解最小生成树中都使用了贪婪算法的思想。
用于求解最优问题。通常已知一些约束和最优函数，满足约束的解决方案成为可行方案（feasible solution）。
最优函数值最大的可行方案称为最优方案。
核心思想#每个步骤都选择最优解，上一步选择的结果在后续步骤中不会改变，即不能撤销前面步骤，同时每个选择都应确保方案可行。
注意：
贪婪算法只在当前最优解即为总体最优解时能正确解出最优解，即local optimum = global optimum的gool luck情况。 贪婪算法不能保证结果一定为最优，但它给出的结果一定与最优结果取值相近，即启发式算法（heuristics）。因此当寻找实际的最优解需要耗费大量时间时，直觉上可以采用贪婪算法。good enough。 活动选择问题#给出需要共用同一资源的一系列时间，记为$S={a_1,a_2,……,a_n}$。每个事件$a_i$发生的时间间隔为$[s_i,f_i)$。
对于事件$a_i$和$a_j$，若有$s_i\ge f_j$或$s_j\ge f_j$，则称为两个事件兼容，即两个事件的时间不重合。
假设：$f_1\le f_2\le……\le f_{n-1}\le f_n$。
目标是选择能够兼容发生的事件的最大子集。
例：
最多能兼容4个事件，为最优解，且最优结果不唯一。
动态规划求解#定义$S_{ij}$为起始时间在$a_i$开始之后且结束时间在$a_j$结束之前的解。
将$S_{ij}$分为两个子问题，进行divide-conquer。
时间复杂度为$O(N^3)$。
贪婪算法求解#不同的贪婪规则：
挑选在不与前面重合的前提下，最早开始的事件。
挑选在不与前面重合的前提下，时间最短的事件。
挑选在不与前面重合的前提下，与其余事件冲突最少的事件。
挑选在不与前面重合的前提下，最早结束的事件。即尽快使资源得到释放。
正确性证明#需要证明：①算法的结果满足时间不重合。 ②结果最优。
定理
对于任意非空的子问题$S_k$，若$a_m$是$S_k$中最早结束的事件，则$a_m$一定是$S_k$最优解的集合中的元素。（最优解不一定唯一）
证明：
设$A_k$为某一最优解集合（不一定包括$a_m$），设$a_{ef}$为某个最优解集合中结束时间最早的一个事件。
①若$a_m$与$a_{ef}$是同一个事件，则无需额外证明。
②否则，用$a_m$替换$a_{ef}$，得到新的集合$A_k&amp;rsquo;$。由于$a_m$是整个集合中最早结束的事件，因此有$f_m\le f_{ef}$。
显然$a_m$与这一解集合中的其他事件都不冲突，因此$A_k&amp;rsquo;$是一个新的最优解集合。
最终解决办法
每次选择最先结束的事件，尾递归可以转换为迭代求解，因此总的时间复杂度为：$O(NlogN)$。
问题：能否将“最先结束”这一条件替换为“最迟开始”？
答：可以，能够通过证明。
另一种DP求解#其中$c_{1,j}$表示从第一个事件到第j个事件间的最优解，$k(j)$表示在$a_j$开始前结束且距离$a_j$最近的事件。
相较于第一种动态规划，在证明了上述定理的基础上，把i、j双重循环简化为单层循环。
$c_{1,j-1}$表示不选取$a_j$时的最优解，$c_{1,k(j)}$表示在$a_j$之前与$a_j$不冲突的最优解，$c_{1,k(j)}+1$表示选取$a_j$时的最优解。
如果修改问题为：每个事件带有一定权重，而最优解指的是所有事件权重之和最大的解
此时动态规划算法依然是正确的，但贪婪算法不一定正确。
贪婪策略的要素#将最优问题分步骤解决，使得每步中需要做出一个选择。 需要证明，总存在至少一个最优解包含贪婪算法的选择（与上述正确性证明类似），这保证了贪婪算法是安全可行的。 需要证明，存在最优子结构。即用贪婪算法选择出的最优子解+剩余部分的最优子解=原问题的最优解。 霍夫曼编码问题#问题描述#霍夫曼编码通常用于文件压缩。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E8%BF%91%E4%BC%BC%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E8%BF%91%E4%BC%BC%E7%AE%97%E6%B3%95/</guid>
      <description>近似算法（Approximation）#概述#用于解决困难问题，如NPC问题。
对于NPC问题的处理方法：
N很小，$O(2^N)$也可以接受。
总体问题很复杂，但可以在多项式时间解决一些重要的特殊情况。
寻找一个能在多项式时间得出的次优解（near-optimal solution）。→ 渐近算法
近似比例（approximation radio）#定义#若对于任意大小的输入$n$，渐近算法求解的开销$C$不超过最优解开销$C*$的$\rho (n)$因子，则该算法的渐近比例为$\rho(n)$。称为$\rho(n)$渐近算法。
近似方案（approximation scheme）#一个最优解问题的渐近方案是一个输入为问题instance和一个正数$\epsilon$的渐近算法，使其对于任何确定的$\epsilon$，都是一个$(1+\epsilon)$渐近算法。
若对于任意确定的$\epsilon &amp;gt;0$，渐近方案能在多项式时间内得出结果，则称为PTAS（polynomial-time approximation scheme）。
通常PTAS的时间复杂度形如$O(n^{2/\epsilon})$。随着$\epsilon$减小，时间复杂度增大。
有一种特殊情况复杂度形如$O((1/\epsilon)^2n^3)$，优于上面的情况，这种PTAS称为FPTAS（fully polynomial-time）。
fully表示时间复杂度中的两个变量$n$和$\epsilon$都是多项式形式。
实例1：装箱问题#Bin Packing，NP-Hard问题。
给出大小分别为$S_1,S_2,……S_N$的$N$个物品，每个$S_i$都有$0&amp;lt;S_i\le 1$。每个背包有单位空间，将这些物品装入数量最少的背包里。
decision version： 给出K个背包和N个物品的大小，求解是否能装下。
NP-Compelete问题。
Next Fit#依次遍历每个物品，若上一个背包能放下则放入上一个背包，否则新开一个背包。线性时间复杂度。
定理：
若$M$为物品集$I$的最优解（即最少背包数），则Next Fit近似算法的结果一定不超过$2M-1$，且存在解法使得结果刚好等于$2M-1$。
证明：
反证法：即证明若NF算法结果为$2M$或$2M+1$，则最优解的结果一定大于等于$M+1$。
近似比例为2，需要更好的方案。
First Fit#搜索第一个满足能装下该物品的背包，若存在，则放入该背包中；若不存在，则新开一个背包。时间复杂度为$O(NlogN)$。
定理：
若$M$为物品集$I$的最优解（即最少背包数），则First Fit近似算法的结果一定不超过$17M/10$，且存在解法使得结果刚好等于$17(M-1)/10$。
近似比例为1.7。
Best Fit#将物品放置在“最拥挤”的背包里，即放下物品后剩余空间最小。时间复杂度为$O(NlogN)$。结果同样$\le 1.7M$。
Worst Fit#将物品放置在“最空闲”的背包里，即放下物品后剩余空间最大。
对比#另一个例子：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E9%9A%8F%E6%9C%BA%E5%8C%96%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ArcticPirateL.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ADS/%E7%AE%97%E6%B3%95/%E9%9A%8F%E6%9C%BA%E5%8C%96%E7%AE%97%E6%B3%95/</guid>
      <description>随机化算法（Randomized Alogorithms）#随机化的定义#the world behaves randomly：随机生成乱序的输入并使用传统算法解决 $\Rightarrow$ 平均情况分析（Average-case Analysis）。 the algorithm behaves randomly：当输入情况最坏时算法采用随机决策的方式求解。 两种随机化目标：
efficient algorithm： yield the correct answer with ==high probability==.注重效率高。 deterministic algorithm： always correct and run efficiently ==in expectation==.注重正确性。 概率前置知识#$Pr[A]$：事件$A$发生的概率。
$\overline A$：事件$A$的补集，即不发生。
$E[X]$：随机变量$X$的期望（平均值）。
$E[X]=\Sigma_{j=0}^{\infin}j\cdot Pr[X=j]$。
实例1：Hiring问题#雇佣一个助手，共$N$天，每天面试一个人。
面试开销：$C_i &amp;laquo; $雇佣开销：$C_h$。
分析开销，假设$N$天面试雇佣了$M$个应聘者，则总开销为：$O(NC_i+MC_h)$。目标为使总开销最小化。
原始解法#从0开始，每次遇到更好的应聘者，解雇此前最好的，雇佣当前的。
最坏情况：候选人的质量按升序排列。此时开销约为$O(NC_h)$。
最好情况：候选人的质量按降序排列。
改进方向：
提高解决最坏情况的效率。 尝试避免出现最坏情况。 若候选人随机顺序出现。设$X$为雇佣次数，找到$X$的期望。核心问题是如何知道$Pr[X=j]$。
定义$X_i=\begin{cases} 1 &amp;amp; ifcandidateiishired\ 0 &amp;amp; ifcandidateiisNOThired \end{cases} \Rightarrow X=\Sigma_{i=1}^NX_i$ 得到：</description>
    </item>
    
  </channel>
</rss>
